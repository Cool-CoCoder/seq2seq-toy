{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "en = [['<b>', 'i', ' ', 'like', ' ', 'you', '<e>'], ['<b>', 'i', ' ', 'hate', ' ', 'you', '<e>'],\n",
    "      ['<b>', 'i', ' ', 'love', ' ', 'you', '<e>'],\n",
    "      ['<b>', 'he', ' ', 'like', ' ', 'you', '<e>']]\n",
    "zh = [['<b>', '我', '喜欢', '你', '<e>'], ['<b>', '我', '讨厌', '你', '<e>'], ['<b>', '我', '爱', '你', '<e>'],\n",
    "      ['<b>', '他', '喜欢', '你', '<e>']]\n",
    "\n",
    "en_vocab_i2t = ['i', 'like', 'you', 'hate', 'he', ' ', 'love', '<b>', '<e>']\n",
    "en_vocab_t2i = {'i': 0, 'like': 1, 'you': 2, 'hate': 3, 'he': 4, ' ': 5, 'love': 6\n",
    "    , '<b>': 7, '<e>': 8}\n",
    "zh_vocab_i2t = ['我', '喜欢', '你', '讨厌', '爱', ' ', '他', '<b>', '<e>']\n",
    "zh_vocab_t2i = {'我': 0, '喜欢': 1, '你': 2, '讨厌': 3, '爱': 4, ' ': 5, '他': 6\n",
    "    , '<b>': 7, '<e>': 8}\n",
    "\n",
    "\n",
    "def process(en, zh):\n",
    "    en_idx = [[en_vocab_t2i[token] for token in line] for line in en]\n",
    "    zh_idx = [[zh_vocab_t2i[token] for token in line] for line in zh]\n",
    "    return torch.tensor(en_idx), torch.tensor(zh_idx)\n",
    "\n",
    "\n",
    "en_idx, zh_idx = process(en, zh)\n",
    "num_hiddens = 32\n",
    "num_layers = 2\n",
    "embed_size = 4\n",
    "zh_vocab_size = len(zh_vocab_i2t)\n",
    "en_vocab_size = len(en_vocab_i2t)\n",
    "\n",
    "\n",
    "class Seq2SeqEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.LSTM(embed_size, num_hiddens, num_layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        X = self.embed(input)\n",
    "        X = X.permute(1, 0, 2)\n",
    "        out_put, (last_hiddens, last_cells) = self.rnn(X)\n",
    "        return out_put, last_hiddens, last_cells\n",
    "\n",
    "\n",
    "class Seq2seqDecoder(nn.Module):\n",
    "    # vocab_size 为 tgt的\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.LSTM(num_hiddens + embed_size, num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, input, enc_hiddens, enc_cells):\n",
    "        X = self.embed(input)\n",
    "        X = X.permute(1, 0, 2)\n",
    "        context = enc_hiddens[-1].repeat(X.shape[0], 1, 1)\n",
    "        X_and_context = torch.concat((X, context), 2)\n",
    "        output, (last_hiddens, last_cells) = self.rnn(X_and_context, (enc_hiddens, enc_cells))\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        return output, last_hiddens, last_cells\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基类\"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X):\n",
    "        enc_outputs, enc_h, enc_c = self.encoder(enc_X)\n",
    "        dec_output,dec_h,dec_c  = self.decoder(dec_X, enc_h, enc_c)\n",
    "        return dec_output,dec_h,dec_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "encoder = Seq2SeqEncoder(zh_vocab_size, embed_size, num_hiddens, num_layers)\n",
    "decoder = Seq2seqDecoder(en_vocab_size, embed_size, num_hiddens, num_layers)\n",
    "\n",
    "net = EncoderDecoder(encoder, decoder)\n",
    "y_hat, _, _ = net(zh_idx, en_idx)\n",
    "y_hat.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "en_idx.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 300\n",
    "\n",
    "\n",
    "def train():\n",
    "    for e in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        y_hat, _, _ = net(zh_idx, en_idx)\n",
    "\n",
    "        loss = loss_fn(y_hat.permute(0,2,1), en_idx)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('epoch:{},loss:{}'.format(e, loss))\n",
    "\n",
    "\n",
    "train()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "tgt = torch.tensor(7).unsqueeze(0).unsqueeze(0)\n",
    "x = ['<b>', '我', '讨厌','我','<e>']\n",
    "x = torch.tensor([zh_vocab_t2i[token] for token in x]).unsqueeze(0)\n",
    "x,tgt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "enc_x,h,c = encoder(x)\n",
    "res = \"\"\n",
    "while(1):\n",
    "    output,h,c = decoder(tgt,h,c)\n",
    "    pred = torch.argmax(output,dim=2)\n",
    "    res += en_vocab_i2t[pred]\n",
    "    if pred == en_vocab_t2i['<e>']:\n",
    "        break\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
